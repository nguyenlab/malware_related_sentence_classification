print('Importing default libraries...')
import json
from pathlib import Path
import sys
import re
import time

print('Importing external libraries...')
from nltk.corpus import stopwords
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC


# Testing/Running options
testing = 0  # If testing = 1, test the program on a small data.
n_training_examples = 1000  # The amount of data of the small data.

# Features concatenation options
BOW = 1  # Whether to include the bag-of-words feature.
GAT_weak_label = 1  # Whether to include the bag-of-words feature.
MAEC_weights = 1  # Whether to include the bag-of-words feature.

# Printing options
print_y_pred = 0

# Global variables
stop_words = stopwords.words('english')

# Constants
CATEGORIES = ['ActionName', 'Capability', 'StrategicObjectives', 'TacticalObjectives']
N_LABELS = {
    'ActionName': 211, 
    'Capability': 20, 
    'StrategicObjectives': 65, 
    'TacticalObjectives': 148,
}
N_TRAIN_SENTENCES = 9424
N_DEV_SENTENCES = 1213
N_TEST_SENTENCES = 618


def read_input_data(data_dir):
    print('Reading input data...')

    # Read train data: sentences and labels
    train_file = data_dir / 'train.tsv'
    train_sentences = []
    train_labels = []
    with open(train_file) as fin:
        for line in fin:
            tokens = line[: -1].split('\t')
            train_sentences.append(tokens[0])
            train_labels.append(int(tokens[1]))
    train_labels = np.array(train_labels)
    print('\tlen(train_sentences)', len(train_sentences))
    print('\ttrain_labels.shape', train_labels.shape)

    # Read dev data: sentences and labels
    dev_file = data_dir / 'dev.tsv'
    dev_sentences = []
    dev_labels = []
    with open(dev_file) as fin:
        for line in fin:
            tokens = line[: -1].split('\t')
            dev_sentences.append(tokens[0])
            dev_labels.append(int(tokens[1]))
    dev_labels = np.array(dev_labels)
    print('\tlen(dev_sentences)', len(dev_sentences))
    print('\tdev_labels.shape', dev_labels.shape)

    # Read test data: sentences
    test_file = data_dir / 'test.txt'
    test_sentences = []
    with open(test_file) as fin:
        for line in fin:
            test_sentences.append(line[: -1])
    print('\tlen(test_sentences)', len(test_sentences))

    # Read test data: labels
    label_file = data_dir / 'test.labels'
    test_labels = []
    with open(label_file) as fin:
        for line in fin:
            test_labels.append(int(line[0]))
    test_labels = np.array(test_labels)
    print('\tlen(test_labels)', len(test_labels))

    return train_sentences, train_labels, dev_sentences, dev_labels, test_sentences, test_labels


def clean(sentence):
    sentence = sentence.lower()
    sentence = re.sub('[^\w]', ' ', sentence)
    sentence = ' '.join(sentence.split())
    return sentence


def remove_stopwords(sentence):
    sentence = ' '.join([token for token in sentence.split() if token not in stop_words])
    return sentence if sentence != '' else '1'  # if the sentence is removed totally, we set the sentence as the string '1'


def process_data(data_list):
    for sentences in data_list:
        for i in range(len(sentences)):
            sentences[i] = clean(sentences[i])
            sentences[i] = remove_stopwords(sentences[i])


def evaluate(data_type, clf, features, labels):
    result = '\t[%5s: ' % data_type
    y_pred = clf.predict(features)
    tn, fp, fn, tp = confusion_matrix(labels, y_pred).ravel()
    result += '(%4d, %4d, %4d, %4d)\t' % (tp, fp, tn, fn)
    pre, rec, f1, _ = precision_recall_fscore_support(labels, y_pred, average='binary', zero_division=0)
    result += '(%.5f, %.5f, %.5f)]\t' % (pre, rec, f1)
    return result, y_pred


def extract_maec_keywords(maec_keywords_file):
    def get_category(n):
        n += 1
        stt = 0
        for category in CATEGORIES:
            stt += N_LABELS[category]
            if n <= stt:
                return category
        print('ERROR')

    def extract_single_keywords(line):
        line = clean(line)
        return line.split()

    maec_single_keywords = dict()  # Format: maec_single_keywords['ActionName'] = [['kw1A', 'kw1B'], ['kw2A', 'kw2B'], [...], [...]]
    for key in N_LABELS:
        maec_single_keywords[key] = []
    with open(maec_keywords_file) as fin:
        for i, line in enumerate(fin):
            maec_single_keywords[get_category(i)].append(extract_single_keywords(line))
    return maec_single_keywords


def get_maec_classes_features(train_sentences, dev_sentences, test_sentences, maec_single_keywords):
    def judge(sentence, keywords):
        cnt = 0
        for keyword in keywords:
            if keyword in sentence:
                cnt += 1
        return cnt / len(keywords)

    def get_keyword_matching_features(sentences, maec_keywords):
        ret = []
        for sentence in sentences:
            sentence = clean(sentence)
            features = []
            for category in CATEGORIES:
                for keywords in maec_keywords[category]:
                    features.append(judge(sentence, keywords))
            ret.append(features)
        return np.array(ret)

    def get_keyword_matching_featuers():
        train_features = get_keyword_matching_features(train_sentences, maec_single_keywords)
        dev_features = get_keyword_matching_features(dev_sentences, maec_single_keywords)
        test_features = get_keyword_matching_features(test_sentences, maec_single_keywords)
        return train_features, dev_features, test_features

    train_features, dev_features, test_features = get_keyword_matching_featuers()
    return train_features, dev_features, test_features


def to_np_array_and_expand_dims(arr):
    return np.expand_dims(np.array(arr), axis=1)


def read_gat_label_features(gat_file):
    ret = dict()
    ret['train'] = []
    ret['dev'] = []
    ret['test'] = []
    with open(gat_file) as fin:
        for line in fin:
            tokens = line.split()
            assert len(tokens) == 3, print(tokens)
            assert int(tokens[1]) == len(ret[tokens[0]])
            ret[tokens[0]].append(int(tokens[2]))
    return to_np_array_and_expand_dims(ret['train']), to_np_array_and_expand_dims(ret['dev']), to_np_array_and_expand_dims(ret['test'])
    

def get_param_grid(param_grid_setting):
    if param_grid_setting == 'first_grid_search':
        n_vocabs = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 12975]
        Cs = [0.001, 0.01, 0.1, 1, 5, 10, 50, 100, 1000]
        gammas = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]
    elif param_grid_setting == 'second_grid_search':
        n_vocabs = [2000]
        Cs = [1, 2, 3, 4, 5]
        gammas = [0.00001, 0.00003, 0.00005, 0.00007, 0.00009]
    elif param_grid_setting == 'best_setting':
        n_vocabs = [2000]
        Cs = [1]
        gammas = [0.00009]
    else:
        print('param_grid_setting should be "first_grid_search", "second_grid_search", or "best_setting".')
        raise 
    return n_vocabs, Cs, gammas


def main(data_dir, gat_file, maec_keywords_file, param_grid_setting):
    # Read input data
    train_sentences, train_labels, dev_sentences, dev_labels, test_sentences, test_labels = read_input_data(data_dir)
    sys.stdout.flush()

    # Process input data
    print('Processing input data...')
    sys.stdout.flush()
    process_data([train_sentences, dev_sentences, test_sentences])

    # Get vocab size from corpus
    print('Getting vocab size...')
    sys.stdout.flush()
    vectorizer = CountVectorizer()
    vectorizer.fit(train_sentences)
    vocab_size = len(vectorizer.vocabulary_)
    print('Vocabulary size:', vocab_size)
    sys.stdout.flush()

    if MAEC_weights:
        # Extract MAEC keywords
        print('Extracting MAEC keywords...')
        sys.stdout.flush()
        maec_single_keywords = extract_maec_keywords(maec_keywords_file)

        # Get MAEC classes features
        print('Getting MAEC classes features...')
        sys.stdout.flush()
        train_maec_features, dev_maec_features, test_maec_features = get_maec_classes_features(train_sentences, dev_sentences, test_sentences, maec_single_keywords)
        print('\ttrain_maec_features.shape', train_maec_features.shape)
        print('\tdev_maec_features.shape', dev_maec_features.shape)
        print('\ttest_maec_features.shape', test_maec_features.shape)
        sys.stdout.flush()

    print()
    sys.stdout.flush()

    n_vocabs, Cs, gammas = get_param_grid(param_grid_setting)
    for n_vocab in n_vocabs:
        print(f'---------------- max_features = {n_vocab} ----------------')
        sys.stdout.flush()

        print('Fitting CountVectorizer model...')
        sys.stdout.flush()
        vectorizer = CountVectorizer(max_features=n_vocab)
        vectorizer.fit(train_sentences)

        print('CountVectorizer transforming...')
        sys.stdout.flush()
        train_features = vectorizer.transform(train_sentences).toarray()
        dev_features = vectorizer.transform(dev_sentences).toarray()
        test_features = vectorizer.transform(test_sentences).toarray()
        print('\ttrain_features.shape', train_features.shape)
        print('\tdev_features.shape', dev_features.shape)
        print('\ttest_features.shape', test_features.shape)
        sys.stdout.flush()

        if GAT_weak_label:
            print('Concatenating GAT label features...')
            sys.stdout.flush()
            train_gat_label_features, dev_gat_label_features, test_gat_label_features = read_gat_label_features(gat_file)
            train_features = np.concatenate((train_features, train_gat_label_features), axis=1)
            dev_features = np.concatenate((dev_features, dev_gat_label_features), axis=1)
            test_features = np.concatenate((test_features, test_gat_label_features), axis=1)
            print('\tConcatenated: train_features.shape', train_features.shape)
            print('\tConcatenated: dev_features.shape', dev_features.shape)
            print('\tConcatenated: test_features.shape', test_features.shape)
            sys.stdout.flush()

        if MAEC_weights:
            print('Concatenating MAEC classes features...')
            sys.stdout.flush()
            train_features = np.concatenate((train_features, train_maec_features), axis=1)
            dev_features = np.concatenate((dev_features, dev_maec_features), axis=1)
            test_features = np.concatenate((test_features, test_maec_features), axis=1)
            print('\tConcatenated: train_features.shape', train_features.shape)
            print('\tConcatenated: dev_features.shape', dev_features.shape)
            print('\tConcatenated: test_features.shape', test_features.shape)
            sys.stdout.flush()

        print()
        sys.stdout.flush()

        for C in Cs:
            for gamma in gammas:
                print(f'*** C = {C}, gamma = {gamma} ***')
                print('SVM fitting...')
                sys.stdout.flush()
                start_time = time.time()
                clf = make_pipeline(StandardScaler(), SVC(C=C, gamma=gamma, probability=True))
                if testing:
                    clf.fit(train_features[: n_training_examples], train_labels[: n_training_examples])
                else:
                    clf.fit(train_features, train_labels)
                print('\tFitting time: %.5s seconds' % (time.time() - start_time))
                sys.stdout.flush()

                print('Evaluating...')
                sys.stdout.flush()
                result, _ = evaluate('dev', clf, dev_features, dev_labels)
                print(result)
                sys.stdout.flush()
                result, y_pred = evaluate('test', clf, test_features, test_labels)
                print(result)
                if print_y_pred:
                    print(len(y_pred))
                    for label in y_pred:
                        print(label, end=' ')
                    print()

                print()
                sys.stdout.flush()


if __name__ == '__main__':
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument('--data_dir', default='./data/processed/data_files')
    p.add_argument('--gat_file', default='./data/processed/gat_output/label_predictions.txt')
    p.add_argument('--maec_keywords_file', default='./data/original/MalwareTextDBv2.0/annotation_guidelines/MAEC_keywords.txt')
    p.add_argument('--param_grid_setting', default='second_grid_search')
    args = p.parse_args()

    main(Path(args.data_dir), args.gat_file, args.maec_keywords_file, args.param_grid_setting)
