"""Gets data for Task 1 from '.tokens' files in 'tokenized_dir'. 'data_type' is in ['train', 'dev'], respective with 'tokenized_dir'.
If 'write' is True, write the data to file '<data_type>.tsv' in the 'output_dir'.
The output file contains many lines, each line is in format: '<sentence><tab><label>'.
"""

import os
from pathlib import Path


def sort_by_name(filenames):
    """Sorts filenames ascending (case insensitive)"""
    foo = [(filename.lower(), filename) for filename in filenames]
    foo.sort()
    return [bar[1] for bar in foo]


def list_dir(directory):
    """Lists all filenames in the a directory (ignore files starting with a dot)"""
    for _, _, files in os.walk(directory): break
    return [e for e in files if not e.startswith('.')]


def split_sentence_annotations(token_annotations, delimiter=''):
    sentence_annotations = []
    sentence_annotation = []
    for token_annotation in token_annotations:
        if token_annotation == delimiter:
            sentence_annotations.append(sentence_annotation)
            sentence_annotation = []
        else:
            sentence_annotation.append(tuple(token_annotation.split()))
    return sentence_annotations


def get_data_in_dot_tokens_file(dot_tokens_file):
    """From a BIO file, gets data for Task 1 in format '<sentence><tab><label>'."""
    ret = []
    lines = []
    with open(dot_tokens_file) as fin:
        # In '.tokens' file, as observed, there are delimiters in form '\n \n' instead of '\n\n'. In 
        # here, handle such cases along with regular cases ('\n\n'), and the case of redundant tailing end line.
        for line in fin:
            tokens = line.split()
            if not len(tokens) in [0, 2]:
                print('WARNING: In file "%s", there exists line "%s" whose number of tokens is not in [0, 2]' % (dot_tokens_file, line[: -1]))
            lines.append(' '.join(tokens))  # now, a line in 'lines' does not contain end line
    
    sentence_annotations = split_sentence_annotations(lines, delimiter='')
    for sentence_annotation in sentence_annotations:
        sentence_label = 0
        for _, token_label in sentence_annotation:
            if token_label != 'O':
                sentence_label = 1
                break
        sentence = ' '.join([token for token, _ in sentence_annotation])
        ret.append((sentence, sentence_label))
    return ret


def get_data(tokenized_dir, data_type, output_dir, write=True):
    """Gets data for Task 1 from '.tokens' files in 'tokenized_dir'. 'data_type' is in ['train', 'dev'], respective with 'tokenized_dir'.
    If 'write' is True, write the data to file '<data_type>.tsv' in the 'output_dir'.
    The output file contains many lines, each line is in format: '<sentence><tab><label>'.
    """
    Path.mkdir(output_dir, parents=True, exist_ok=True)
    all_files = sort_by_name(list_dir(tokenized_dir))
    dot_tokens_files = [file for file in all_files if file.endswith('.tokens')]
    print('----- In folder "%s", there are %d ".tokens" files. -----' % (tokenized_dir, len(dot_tokens_files)))

    ret = []
    for dot_tokens_file in dot_tokens_files:
        print('Processing file "%s"' % dot_tokens_file)
        ret += get_data_in_dot_tokens_file(tokenized_dir / dot_tokens_file)

    if write:
        with open(output_dir / (data_type + '.tsv'), 'w') as fout:
            for sentence, label in ret:
                print('%s\t%s' % (sentence, label), file=fout)

    print()
    return ret


def main(malwaretextdbv20_dir, output_dir):
    get_data(malwaretextdbv20_dir / 'data' / 'train' / 'tokenized', 'train', output_dir, write=True)
    get_data(malwaretextdbv20_dir / 'data' / 'dev' / 'tokenized', 'dev', output_dir, write=True)


if __name__ == '__main__':
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument('--malwaretextdbv20_dir', default='./data/original/MalwareTextDBv2.0')
    p.add_argument('--output_dir', default='./data/processed/data_files')
    args = p.parse_args()

    main(Path(args.malwaretextdbv20_dir), Path(args.output_dir))
